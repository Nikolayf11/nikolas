{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "\n",
    "sber_data = pd.read_csv('data_2/sber_data.csv', sep=',')\n",
    "#display(sber_data.head())\n",
    "#Чему равно число строк в таблице?\n",
    "#display(sber_data.tail())\n",
    "\n",
    "#Сколько районов Москвы и Московской области представлено в данных?\n",
    "#display(sber_data['sub_area'].value_counts())\n",
    "\n",
    "#Чему равна максимальная цена квартир (price_doc)? Введите это число полностью, без округлений.\n",
    "#display(sber_data['price_doc'].max())\n",
    "\n",
    "\n",
    "#Проверим, влияет ли уровень экологической обстановки в районе на цену квартиры.\n",
    "# Постройте коробчатую диаграмму цен на квартиры (price_doc) в зависимости от уровня экологической обстановки в районе (ecology).\n",
    "# Какой уровень ценится на рынке меньше всего?\n",
    "#Строим график распределения возраста в зависимости от лояльности\n",
    "fig = px.box(\n",
    "    sber_data,\n",
    "    x='price_doc',\n",
    "    y='ecology',\n",
    "    color='ecology',\n",
    "    orientation = 'h',\n",
    "    labels={'price_doc':'Цены на квартиры', 'ecology':'Уровень экологической обстановки'},\n",
    "    height = 800, \n",
    "    width=1200,\n",
    "    title = \"Распределение цен на квартиры взависимости от уровеня экологической обстановки\",\n",
    ")\n",
    "#fig.show()\n",
    "\n",
    "\n",
    "#Постройте диаграмму рассеяния, которая покажет, как цена на квартиру (price_doc) связана с расстоянием до центра Москвы (kremlin_km). \n",
    "#Выберите все верные утверждения.\n",
    "fig = px.scatter(\n",
    "    sber_data,\n",
    "    x='kremlin_km',\n",
    "    y='price_doc',\n",
    "    labels={'price_doc':'Цены на квартиры, млн.', 'kremlin_km':'Hасстоянием до центра Москвы, км.'},\n",
    "    height = 800, \n",
    "    width=1200,\n",
    "    title = \"Lиаграмму рассеяния зависимости цены на квартиру и расстояния до центра Москвы\",\n",
    ")\n",
    "#fig.show()\n",
    "\n",
    "\n",
    "\n",
    "#В библиотеке pandas специально для этого реализован метод isnull().\n",
    "# Этот метод возвращает новый DataFrame, в ячейках которого стоят булевы значения True и False.\n",
    "# True ставится на месте, где ранее находилось значение NaN.\n",
    "#display(sber_data.isnull().tail())\n",
    "\n",
    "\n",
    "#Первый способ — это вывести на экран названия столбцов, где число пропусков больше 0/\n",
    "#Для этого вычислим средний по столбцам результат метода isnull(). Получим долю пропусков в каждом столбце.\n",
    "#Умножаем на 100 %, находим столбцы, где доля пропусков больше 0, сортируем по убыванию и выводим результат:\n",
    "cols_null_percent = sber_data.isnull().mean() * 100\n",
    "cols_with_null = cols_null_percent[cols_null_percent>0].sort_values(ascending=False)\n",
    "#display(cols_with_null)\n",
    "\n",
    "\n",
    "#Можно воспользоваться столбчатой диаграммой, чтобы визуально оценить соотношение числа пропусков к числу записей.\n",
    "# Самый быстрый способ построить её — использовать метод plot():\n",
    "#cols_with_null.plot(\n",
    "#    kind='bar',\n",
    "#    figsize=(10, 4),\n",
    "#    title='Распределение пропусков в данных'\n",
    "#);\n",
    "\n",
    "\n",
    "\n",
    "#Для создания такой тепловой карты можно воспользоваться результатом метода isnull().\n",
    "# Ячейки таблицы, в которых есть пропуск, будем отмечать жёлтым цветом, а остальные — синим.\n",
    "# Для этого создадим собственную палитру цветов тепловой карты с помощью метода color_pallete() из библиотеки seaborn.\n",
    "#colors = ['blue', 'yellow'] \n",
    "#fig = plt.figure(figsize=(10, 4))\n",
    "#cols = cols_with_null.index\n",
    "#ax = sns.heatmap(\n",
    "#    sber_data[cols].isnull(),\n",
    "#    cmap=sns.color_palette(colors),\n",
    "#)\n",
    "\n",
    "\n",
    "#редварительно создадим копию исходной таблицы — drop_data, чтобы не повредить её.\n",
    "#Зададимся порогом в 70 %: будем оставлять только те столбцы, в которых 70 и более процентов записей не являются пустыми.\n",
    "#После этого удалим записи, в которых содержится хотя бы один пропуск.\n",
    "#Наконец, выведем информацию о числе пропусков и наслаждаемся нулями. \n",
    "#создаем копию исходной таблицы\n",
    "drop_data = sber_data.copy()\n",
    "#задаем минимальный порог: вычисляем 70% от числа строк\n",
    "thresh = drop_data.shape[0]*0.7\n",
    "#удаляем столбцы, в которых более 30% (100-70) пропусков\n",
    "drop_data = drop_data.dropna(thresh=thresh, axis=1)\n",
    "#удаляем записи, в которых есть хотя бы 1 пропуск\n",
    "drop_data = drop_data.dropna(how='any', axis=0)\n",
    "#отображаем результирующую долю пропусков\n",
    "drop_data.isnull().mean()\n",
    "\n",
    "#print(drop_data.shape)\n",
    "\n",
    "\n",
    "\n",
    "#Вся сложность заключается в выборе метода заполнения.\n",
    "# Важным фактором при выборе метода является распределение признаков с пропусками.\n",
    "# Давайте выведем их на экран. \n",
    "# pandas это можно сделать с помощью метода hist():\n",
    "#cols = cols_with_null.index\n",
    "#sber_data[cols].hist(figsize=(20, 8));\n",
    "\n",
    "\n",
    "#Заполнение значений осуществляется с помощью метода fillna().\n",
    "# Главный параметр метода — value (значение, на которое происходит заполнение данных в столбце).\n",
    "# Если метод вызывается от имени всего DataFrame, то в качестве value можно использовать словарь,\n",
    "# где ключи — названия столбцов таблицы, а значения словаря — заполняющие константы. \n",
    "#Создадим такой словарь, соблюдая рекомендации, приведённые выше, а также копию исходной таблицы.\n",
    "# Произведём операцию заполнения с помощью метода fillna() и удостоверимся, что пропусков в данных больше нет:\n",
    "#создаем копию исходной таблицы\n",
    "fill_data = sber_data.copy()\n",
    "#создаем словарь имя столбца: число(признак) на который надо заменить пропуски\n",
    "values = {\n",
    "    'life_sq': fill_data['full_sq'],\n",
    "    'metro_min_walk': fill_data['metro_min_walk'].median(),\n",
    "    'metro_km_walk': fill_data['metro_km_walk'].median(),\n",
    "    'railroad_station_walk_km': fill_data['railroad_station_walk_km'].median(),\n",
    "    'railroad_station_walk_min': fill_data['railroad_station_walk_min'].median(),\n",
    "    'hospital_beds_raion': fill_data['hospital_beds_raion'].mode()[0],\n",
    "    'preschool_quota': fill_data['preschool_quota'].mode()[0],\n",
    "    'school_quota': fill_data['school_quota'].mode()[0],\n",
    "    'floor': fill_data['floor'].mode()[0]\n",
    "}\n",
    "#заполняем пропуски в соответствии с заявленным словарем\n",
    "fill_data = fill_data.fillna(values)\n",
    "#выводим результирующую долю пропусков\n",
    "fill_data.isnull().mean()\n",
    "\n",
    "\n",
    "#Посмотрим, на то, как изменились распределения наших признаков:\n",
    "#cols = cols_with_null.index\n",
    "#fill_data[cols].hist(figsize=(20, 8));\n",
    "#Обратите внимание на то, как сильно изменилось распределение для признака hospital_beds_raion.\n",
    "# Это связано с тем, что мы заполнили модальным значением почти 47 % общих данных.\n",
    "# В результате мы кардинально исказили исходное распределение признака, что может плохо сказаться на модели.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Заполнение недостающих значений константами с добавлением индикатора\n",
    "# Посмотрим на реализацию. Как обычно, создадим копию indicator_data исходной таблицы.\n",
    "# В цикле пройдёмся по столбцам с пропусками и будем добавлять в таблицу новый признак (с припиской \"was_null\"), который получается из исходного с помощью применения метода isnull().\n",
    "# После чего произведём обычное заполнение пропусков, которое мы совершали ранее, и выведем на экран число отсутствующих значений в столбце, чтобы убедиться в результате:\n",
    "#создаем копию исходной таблицы\n",
    "indicator_data = sber_data.copy()\n",
    "#в цикле пробегаемся по названиям столбцов с пропусками\n",
    "for col in cols_with_null.index:\n",
    "    #создаем новый признак-индикатор как col_was_null\n",
    "    indicator_data[col + '_was_null'] = indicator_data[col].isnull()\n",
    "#создаем словарь имя столбца: число(признак) на который надо заменить пропуски   \n",
    "values = {\n",
    "    'life_sq': indicator_data['full_sq'],\n",
    "    'metro_min_walk': indicator_data['metro_min_walk'].median(),\n",
    "    'metro_km_walk': indicator_data['metro_km_walk'].median(),\n",
    "    'railroad_station_walk_km': indicator_data['railroad_station_walk_km'].median(),\n",
    "    'railroad_station_walk_min': indicator_data['railroad_station_walk_min'].median(),\n",
    "    'hospital_beds_raion': indicator_data['hospital_beds_raion'].mode()[0],\n",
    "    'preschool_quota': indicator_data['preschool_quota'].mode()[0],\n",
    "    'school_quota': indicator_data['school_quota'].mode()[0],\n",
    "    'floor': indicator_data['floor'].mode()[0]\n",
    "}\n",
    "#заполняем пропуски в соответствии с заявленным словарем\n",
    "indicator_data = indicator_data.fillna(values)\n",
    "#выводим результирующую долю пропусков\n",
    "#indicator_data.isnull().mean()\n",
    "#display(indicator_data.head())\n",
    "#Метод исходит из предположения, что, если дать модели информацию о том, что в ячейке ранее была пустота, \n",
    "# то она будет меньше доверять таким записям и меньше учитывать её в процессе обучения. Иногда такие фишки действительно работают, \n",
    "# иногда не дают эффекта, а иногда и вовсе могут ухудшить результат обучения и затруднить процесс обучения.\n",
    "\n",
    "\n",
    "#Комбинирование методов\n",
    "\n",
    "#Наверняка вы уже догадались, что необязательно использовать один метод. Вы можете их комбинировать. Например, мы можем:\n",
    "#удалить столбцы, в которых более 30 % пропусков;\n",
    "#удалить записи, в которых более двух пропусков одновременно;\n",
    "#заполнить оставшиеся ячейки константами.\n",
    "#Посмотрим на реализацию такого подхода в коде:\n",
    "#создаём копию исходной таблицы\n",
    "combine_data = sber_data.copy()\n",
    "\n",
    "#отбрасываем столбцы с числом пропусков более 30% (100-70)\n",
    "n = combine_data.shape[0] #число строк в таблице\n",
    "thresh = n*0.7\n",
    "combine_data = combine_data.dropna(thresh=thresh, axis=1)\n",
    "\n",
    "#отбрасываем строки с числом пропусков более 2 в строке\n",
    "m = combine_data.shape[1] #число признаков после удаления столбцов\n",
    "combine_data = combine_data.dropna(thresh=m-2, axis=0)\n",
    "\n",
    "#создаём словарь 'имя_столбца': число (признак), на который надо заменить пропуски \n",
    "values = {\n",
    "    'life_sq': combine_data['full_sq'],\n",
    "    'metro_min_walk': combine_data['metro_min_walk'].median(),\n",
    "    'metro_km_walk': combine_data['metro_km_walk'].median(),\n",
    "    'railroad_station_walk_km': combine_data['railroad_station_walk_km'].median(),\n",
    "    'railroad_station_walk_min': combine_data['railroad_station_walk_min'].median(),\n",
    "    'preschool_quota': combine_data['preschool_quota'].mode()[0],\n",
    "    'school_quota': combine_data['school_quota'].mode()[0],\n",
    "    'floor': combine_data['floor'].mode()[0]\n",
    "}\n",
    "#заполняем оставшиеся записи константами в соответствии со словарем values\n",
    "combine_data = combine_data.fillna(values)\n",
    "#выводим результирующую долю пропусков\n",
    "#display(combine_data.isnull().mean())\n",
    "\n",
    "#print(combine_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 100.0% уникальных значений\n",
      "oil_chemistry_raion: 99.03% одинаковых значений\n",
      "railroad_terminal_raion: 96.27% одинаковых значений\n",
      "nuclear_reactor_raion: 97.17% одинаковых значений\n",
      "big_road1_1line: 97.44% одинаковых значений\n",
      "mosque_count_1000: 98.08% одинаковых значений\n",
      "Результирующее число признаков: 55\n"
     ]
    }
   ],
   "source": [
    "#Пусть у нас есть признак, по которому мы будем искать выбросы.\n",
    "# Давайте рассчитаем его статистические показатели (минимум, максимум, среднее, квантили)\n",
    "# и по ним попробуем определить наличие аномалий.\n",
    "sber_data['life_sq'].describe()\n",
    "\n",
    "#Найдём число квартир с нулевой жилой площадью:\n",
    "#print(sber_data[sber_data['life_sq'] == 0].shape[0])\n",
    "\n",
    "#А теперь выведем здания с жилой площадью более 7 000 квадратных метров:\n",
    "#display(sber_data[sber_data['life_sq'] > 7000])\n",
    "\n",
    "\n",
    "#Логичен вопрос: а много ли у нас таких квартир, у которых жилая площадь больше, чем суммарная?\n",
    "outliers = sber_data[sber_data['life_sq'] > sber_data['full_sq']]\n",
    "#print(outliers.shape[0])\n",
    "\n",
    "\n",
    "#Таких квартир оказывается 37 штук. Подобные наблюдения уже не поддаются здравому смыслу — они являются ошибочными, \n",
    "# и от них стоит избавиться. Для этого можно воспользоваться методом drop() и удалить записи по их индексам:\n",
    "cleaned = sber_data.drop(outliers.index, axis=0)\n",
    "#print(f'Результирующее число записей: {cleaned.shape[0]}')\n",
    "\n",
    "#Ещё пример: давайте посмотрим на признак числа этажей (floor).\n",
    "#display(sber_data['floor'].describe())\n",
    "\n",
    "#Снова видим подозрительную максимальную отметку в 77 этажей. Проверим все квартиры, которые находятся выше 50 этажей:\n",
    "#display(sber_data[sber_data['floor']> 50])\n",
    "#Всего одна квартира в Ломоносовском районе. Пора идти в интернет в поиске самых высоких зданий в Москве! \n",
    "\n",
    "#На гистограмме мы можем увидеть потенциальные выбросы как низкие далеко отстоящие от основной группы столбцов «пеньки», \n",
    "# а на коробчатой диаграмме — точки за пределами усов\n",
    "\n",
    "#Построим гистограмму и коробчатую диаграмму для признака полной площади (full_sq):\n",
    "#fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 4))\n",
    "#histplot = sns.histplot(data=sber_data, x='full_sq', ax=axes[0]);\n",
    "#histplot.set_title('Full Square Distribution');\n",
    "#boxplot = sns.boxplot(data=sber_data, x='full_sq', ax=axes[1]);\n",
    "#boxplot.set_title('Full Square Boxplot');\n",
    "\n",
    "\n",
    "\n",
    "#В соответствии с этим алгоритмом напишем функцию outliers_iqr(), которая вам может ещё не раз пригодиться в реальных задачах.\n",
    "# Эта функция принимает на вход DataFrame и признак, по которому ищутся выбросы, а затем возвращает потенциальные выбросы,\n",
    "# найденные с помощью метода Тьюки, и очищенный от них датасет.\n",
    "\n",
    "#Квантили вычисляются с помощью метода quantile().\n",
    "# Потенциальные выбросы определяются при помощи фильтрации данных по условию выхода за пределы верхней или нижней границы.\n",
    "\n",
    "#Одним из таких подходов является метод межквартильного размаха (его еще называют методом Тьюки), \n",
    "# который используется для построения коробчатой диаграммы.\n",
    "\n",
    "#вычислить 25-ый и 75-ый квантили (первый и третий квартили) — и  для признака, который мы исследуем;\n",
    "\n",
    "def outliers_iqr(data, feature):\n",
    "    x = data[feature]\n",
    "    quartile_1, quartile_3 = x.quantile(0.25), x.quantile(0.75),\n",
    "    iqr = quartile_3 - quartile_1\n",
    "    lower_bound = quartile_1 - (iqr * 1.5)\n",
    "    upper_bound = quartile_3 + (iqr * 1.5)\n",
    "    outliers = data[(x < lower_bound) | (x > upper_bound)]\n",
    "    cleaned = data[(x >= lower_bound) & (x <= upper_bound)]\n",
    "    return outliers, cleaned\n",
    "\n",
    "#Применим эту функцию к таблице sber_data и признаку full_sq, а также выведем размерности результатов:\n",
    "outliers, cleaned = outliers_iqr(sber_data, 'full_sq')\n",
    "#print(f'Число выбросов по методу Тьюки: {outliers.shape[0]}')\n",
    "#print(f'Результирующее число записей: {cleaned.shape[0]}')\n",
    "#display(outliers.head())\n",
    "#display(cleaned.head())\n",
    "\n",
    "\n",
    "#Согласно классическому методу Тьюки, под выбросы у нас попали 963 записи в таблице.\n",
    "# Давайте построим гистограмму и коробчатую диаграмму на новых данных cleaned_sber_data:\n",
    "\n",
    "#fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 4))\n",
    "#histplot = sns.histplot(data=cleaned, x='full_sq', ax=axes[0]);\n",
    "#histplot.set_title('Cleaned Full Square Distribution');\n",
    "#boxplot = sns.boxplot(data=cleaned, x='full_sq', ax=axes[1]);\n",
    "#boxplot.set_title('Cleaned Full Square Boxplot');\n",
    "\n",
    "\n",
    "#Построим две гистограммы признака расстояния до МКАД (mkad_km): первая — в обычном масштабе, а вторая — в логарифмическом.\n",
    "# Логарифмировать будем с помощью функции log() из библиотеки numpy (натуральный логарифм — логарифм по основанию числа e).\n",
    "# Признак имеет среди своих значений 0. Из математики известно, что логарифма от 0 не существует, поэтому мы прибавляем к\n",
    "# нашему признаку 1, чтобы не логарифмировать нули и не получать предупреждения.\n",
    "#fig, axes = plt.subplots(1, 2, figsize=(15, 4))\n",
    "\n",
    "#гистограмма исходного признака\n",
    "#histplot = sns.histplot(sber_data['mkad_km'], bins=30, ax=axes[0])\n",
    "#histplot.set_title('MKAD Km Distribution');\n",
    "\n",
    "#гистограмма в логарифмическом масштабе\n",
    "#log_mkad_km= np.log(sber_data['mkad_km'] + 1)\n",
    "#histplot = sns.histplot(log_mkad_km , bins=30, ax=axes[1])\n",
    "#histplot.set_title('Log MKAD Km Distribution');\n",
    "\n",
    "\n",
    "#Примечание: Численный показатель асимметрии можно вычислить с помощью метода:\n",
    "#print(log_mkad_km.skew())\n",
    "# -0.14263612203024953\n",
    "\n",
    "\n",
    "#На вход она принимает DataFrame и признак, по которому ищутся выбросы.\n",
    "# В дополнение добавим в функцию возможность работы в логарифмическом масштабе: для этого введём аргумент log_scale.\n",
    "# Если он равен True, то будем логарифмировать рассматриваемый признак, иначе — оставляем его в исходном виде.\n",
    "\n",
    "#Как и раньше, функция будет возвращать выбросы и очищенные от них данные:\n",
    "\n",
    "def outliers_z_score(data, feature, log_scale=False):\n",
    "    if log_scale:\n",
    "        x = np.log(data[feature]+1)\n",
    "    else:\n",
    "        x = data[feature]\n",
    "    mu = x.mean()\n",
    "    sigma = x.std()\n",
    "    lower_bound = mu - 3 * sigma\n",
    "    upper_bound = mu + 3 * sigma\n",
    "    outliers = data[(x < lower_bound) | (x > upper_bound)]\n",
    "    cleaned = data[(x >= lower_bound) & (x <= upper_bound)]\n",
    "    return outliers, cleaned\n",
    "\n",
    "\n",
    "#Применим эту функцию к таблице sber_data и признаку mkad_km, а также выведем размерности результатов:\n",
    "outliers, cleaned = outliers_z_score(sber_data, 'mkad_km', log_scale=True)\n",
    "#print(f'Число выбросов по методу z-отклонения: {outliers.shape[0]}')\n",
    "#print(f'Результирующее число записей: {cleaned.shape[0]}')\n",
    "\n",
    "\n",
    "#Итак, метод z-отклонения нашел нам 33 потенциальных выброса по признаку расстояния до МКАД.\n",
    "# Давайте узнаем, в каких районах (sub_area) представлены эти квартиры:\n",
    "#print(outliers['sub_area'].unique())\n",
    "\n",
    "\n",
    "\n",
    "#озможно, мы не учли того факта, что наш логарифм распределения всё-таки не идеально нормален и в нём присутствует\n",
    "# некоторая асимметрия. Возможно, стоит дать некоторое «послабление» на границы интервалов? Давайте отдельно построим\n",
    "# гистограмму прологарифмированного распределения, а также отобразим на гистограмме вертикальные линии, соответствующие\n",
    "# среднему (центру интервала в методе трёх сигм) и границы интервала . Вертикальные линии можно построить с помощью метода axvline().\n",
    "# Для среднего линия будет обычной, а для границ интервала — пунктирной (параметр ls ='--'):\n",
    "#fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "#log_mkad_km = np.log(sber_data['mkad_km'] + 1)\n",
    "#histplot = sns.histplot(log_mkad_km, bins=30, ax=ax)\n",
    "#histplot.axvline(log_mkad_km.mean(), color='k', lw=2)\n",
    "#histplot.axvline(log_mkad_km.mean()+ 3 * log_mkad_km.std(), color='k', ls='--', lw=2)\n",
    "#histplot.axvline(log_mkad_km.mean()- 3 * log_mkad_km.std(), color='k', ls='--', lw=2)\n",
    "#histplot.set_title('Log MKAD Km Distribution');\n",
    "\n",
    "\n",
    "\n",
    "#Итак, мы графически построили интервал метода трёх сигм поверх нашего распределения.\n",
    "# Он показывает, какие наблюдения мы берём в интервал, а какие считаем выбросами. Легко заметить,\n",
    "# что среднее значение (жирная вертикальная линия) находится левее моды. Это свойство распределений с левосторонней асимметрией.\n",
    "# Также видны наблюдения, которые мы не захватили своим интервалом (небольшой «пенёк» правее верхней границы) — это и есть наши\n",
    "# квартиры из поселений «Роговское» и «Киевский». Очевидно, что если немного (меньше чем на одну сигму) «сдвинуть» верхнюю границу\n",
    "# вправо, то мы захватим эти наблюдения. Давайте сделаем это.\n",
    "\n",
    "def outliers_z_score_mod(data, feature, log_scale=False, left=3, right=3):\n",
    "    if isinstance(data, str):  # Проверка, если передан путь к файлу\n",
    "        data = pd.read_csv(data)\n",
    "    if log_scale:\n",
    "        x = np.log(data[feature]+1)\n",
    "    else:\n",
    "        x = data[feature]\n",
    "    mu = x.mean()\n",
    "    sigma = x.std()\n",
    "    lower_bound = mu - left * sigma\n",
    "    upper_bound = mu + right * sigma\n",
    "    outliers = data[(x < lower_bound) | (x > upper_bound)]\n",
    "    cleaned = data[(x >= lower_bound) & (x <= upper_bound)]\n",
    "    return outliers, cleaned\n",
    "\n",
    "\n",
    "#outliers, cleaned = outliers_z_score_mod(sber_data, 'mkad_km', log_scale=True, left = 3, right = 3.5)\n",
    "#print(f'Число выбросов по методу : {outliers.shape[0]}')\n",
    "#print(f'Результирующее число записей: {cleaned.shape[0]}')\n",
    "\n",
    "\n",
    "#Постройте гистограмму для признака price_doc в логарифмическом масштабе.\n",
    "# А также, добавьте на график линии, отображающие среднее и границы интервала для метода трех сигм. Выберите верные утверждения:\n",
    "#fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "#log_price_doc = np.log(sber_data['price_doc'] + 1)\n",
    "#histplot = sns.histplot(log_price_doc, bins=30, ax=ax)\n",
    "#histplot.axvline(log_price_doc.mean(), color='k', lw=2)\n",
    "#histplot.axvline(log_price_doc.mean()+ 3 * log_price_doc.std(), color='k', ls='--', lw=2)\n",
    "#histplot.axvline(log_price_doc.mean()- 3 * log_price_doc.std(), color='k', ls='--', lw=2)\n",
    "#histplot.set_title('Log price_doc');\n",
    "\n",
    "\n",
    "\n",
    "#Найдите потенциальные выбросы по признаку price_doc с помощью метода z-отклонения. Используйте логарифмический масштаб распределения.\n",
    "# Сделайте «послабление» на 0.7 сигм в обе стороны распределения. Сколько выбросов вы получили?\n",
    "#outliers, cleaned = outliers_z_score_mod(sber_data, 'price_doc', log_scale=True, left = 3.7, right = 3.7)\n",
    "#print(f'Число выбросов по методу : {outliers.shape[0]}')\n",
    "#print(f'Результирующее число записей: {cleaned.shape[0]}')\n",
    "\n",
    "\n",
    "\n",
    "#Добавьте фишку с логарифмированием в свою функцию outliers_iqr_mod(). Добавьте в неё параметр log_scale.\n",
    "# Если он выставлен в True, то производится логарифмирование признака. Примените полученную функцию к признаку price_doc.\n",
    "# Число межквартильных размахов в обе стороны обозначьте как 3. Чему равно число выбросов, полученных таким методом?\n",
    "def outliers_iqr_mod_log(data, feature, log_scale=False, left=1.5, right=1.5):\n",
    "    #if isinstance(data, str):  # Проверка, если передан путь к файлу\n",
    "     #   data = pd.read_csv(data)\n",
    "    if log_scale:\n",
    "        x = np.log(data[feature])\n",
    "    else:\n",
    "        x = data[feature]\n",
    "\n",
    "    \n",
    "    quartile_1, quartile_3 = x.quantile(0.25), x.quantile(0.75)\n",
    "    iqr = quartile_3 - quartile_1\n",
    "    lower_bound = quartile_1 - (iqr * left)\n",
    "    upper_bound = quartile_3 + (iqr * right)\n",
    "\n",
    "    outliers = data[(x < lower_bound) | (x > upper_bound)]\n",
    "    cleaned = data[(x >= lower_bound) & (x <= upper_bound)]\n",
    "    return outliers, cleaned\n",
    "\n",
    "#outliers, cleaned = outliers_iqr_mod_log(sber_data, 'price_doc', log_scale=True, left=3, right=3)\n",
    "#print(f'Число выбросов по методу Тьюки + log: {outliers.shape[0]}')\n",
    "#print(f'Результирующее число записей: {cleaned.shape[0]}')\n",
    "\n",
    "\n",
    "#Обнаружение и ликвидация дубликатов\n",
    "\n",
    "#Проверим, есть у нас такие записи: для этого сравним число уникальных значений в столбце id с числом строк.\n",
    "# Число уникальных значений вычислим с помощью метода nunique():\n",
    "sber_data['id'].nunique() == sber_data.shape[0]\n",
    "\n",
    "\n",
    "#Чтобы отследить дубликаты, можно воспользоваться методом duplicated(), который возвращает булеву маску для фильтрации.\n",
    "# Для записей, у которых совпадают признаки, переданные методу, он возвращает True, для остальных — False.\n",
    "\n",
    "#У метода есть параметр subset — список признаков, по которым производится поиск дубликатов.\n",
    "# По умолчанию используются все столбцы в DataFrame и ищутся полные дубликаты.\n",
    "\n",
    "#Найдём число полных дубликатов таблице sber_data. Предварительно создадим список столбцов dupl_columns,\n",
    "# по которым будем искать совпадения (все столбцы, не включая id). \n",
    "\n",
    "#Создадим маску дубликатов с помощью метода duplicated() и произведём фильтрацию.\n",
    "# Результат заносим в переменную sber_duplicates. Выведем число строк в результирующем DataFrame:\n",
    "\n",
    "dupl_columns = list(sber_data.columns)\n",
    "dupl_columns.remove('id')\n",
    "\n",
    "mask = sber_data.duplicated(subset=dupl_columns)\n",
    "sber_duplicates = sber_data[mask]\n",
    "#print(f'Число найденных дубликатов: {sber_duplicates.shape[0]}')\n",
    "\n",
    "\n",
    "\n",
    "#еперь нам необходимо от них избавиться. Для этого легче всего воспользоваться методом drop_duplicates(),\n",
    "# который удаляет повторяющиеся записи из таблицы. \n",
    "#Создадим новую таблицу sber_dedupped, которая будет версией исходной таблицы, очищенной от полных дубликатов.\n",
    "\n",
    "sber_dedupped = sber_data.drop_duplicates(subset=dupl_columns)\n",
    "#print(f'Результирующее число записей: {sber_dedupped.shape[0]}')\n",
    "\n",
    "\n",
    "#список неинформативных признаков\n",
    "low_information_cols = [] \n",
    "\n",
    "#цикл по всем столбцам\n",
    "for col in sber_data.columns:\n",
    "    #наибольшая относительная частота в признаке\n",
    "    top_freq = sber_data[col].value_counts(normalize=True).max()\n",
    "    #доля уникальных значений от размера признака\n",
    "    nunique_ratio = sber_data[col].nunique() / sber_data[col].count()\n",
    "    # сравниваем наибольшую частоту с порогом\n",
    "    if top_freq > 0.95:\n",
    "        low_information_cols.append(col)\n",
    "        print(f'{col}: {round(top_freq*100, 2)}% одинаковых значений')\n",
    "    # сравниваем долю уникальных значений с порогом\n",
    "    if nunique_ratio > 0.95:\n",
    "        low_information_cols.append(col)\n",
    "        print(f'{col}: {round(nunique_ratio*100, 2)}% уникальных значений')\n",
    "\n",
    "#Итак, мы нашли шесть неинформативных признаков. Теперь можно удалить их с помощью метода drop(),\n",
    "#передав результирующий список в его аргументы.\n",
    "information_sber_data = sber_data.drop(low_information_cols, axis=1)\n",
    "print(f'Результирующее число признаков: {information_sber_data.shape[1]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
